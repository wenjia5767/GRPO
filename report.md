
### 综合项目报告：基于策略梯度的大语言模型数学推理能力微调

#### 1. 引言与项目目标

本报告旨在详细阐述一项关于大型语言模型微调的项目。项目核心目标是利用一种名为**分组归一化策略梯度（Group-Normalized Policy Gradient）**的强化学习方法，对**Qwen2.5-Math-1.5B**模型进行对齐和优化，使其不仅能够准确解决复杂的数学问题，还能严格遵循预先设定的结构化输出格式。整个训练过程是一个高度自动化的、在线策略（On-Policy）的闭环系统，通过模型自身的持续生成与学习来实现性能的迭代提升。

---

#### 2. 项目核心概览

该项目实现了完整的**生成-评估-更新-同步**训练循环，主要包括以下四个核心环节：

1.  **策略生成（Rollout）**：利用当前版本的模型，高效地生成一批包含多种解题方案的训练样本。
2.  **奖励评估（Reward Evaluation）**：对生成的每个方案进行评估，并计算其奖励和优势值，以量化其质量。
3.  **策略更新（Policy Update）**：基于优势值，使用策略梯度算法更新模型的参数，使其更倾向于生成高质量的答案。
4.  **策略同步（Policy Synchronization）**：将更新后的模型参数同步回数据生成引擎，确保下一轮策略生成基于最新的、更优的模型。

---

#### 3. 详细方法论与实现细节

##### 3.1 基础设施与环境配置

项目在软件和硬件层面都进行了精心设计以保障高效运行。

-   **硬件架构**：为了最大化训练效率，项目采用了双GPU并行处理架构。
    -   **GPU 0 (`train_device_id`)**：专用于模型训练。模型以`bfloat16`数据类型加载，并启用了**SDPA**（Scaled Dot-Product Attention）实现，以优化计算速度和显存占用。
    -   **GPU 1 (`gen_device_id`)**：专用于部署高性能**vLLM**推理引擎。vLLM以其卓越的吞吐量，能够快速生成大量文本，极大地加速了数据采集环节。
-   **软件环境**：代码依赖于`torch`、`transformers`、`vllm`等核心库，并使用`matplotlib`进行结果可视化。所有运行相关的日志和成果都存储在一个名为`grpo_run`的固定目录下。

##### 3.2 数据集介绍

本项目的训练和评估均依赖于**GSM8K**数据集。

-   **全称**：GSM8K 是 **Grade School Math 8K** 的缩写，意为包含8000个小学数学问题的数据集。
-   **数据集性质**：该数据集由一系列需要多步骤推理才能解决的数学应用题组成。每个问题都以自然语言文本形式给出，其答案是数值。由于问题要求模型具备逻辑推理能力，并能生成详细的解题过程，因此GSM8K是衡量模型数学和推理能力的一个标准基准，非常适合本项目的训练目标。
-   **数据集划分**：项目使用了该数据集的两个主要部分：
    -   **训练集**：位于`/home/zhangwj/assignment5/data/gsm8k/train.jsonl`，包含大部分问题，用于在强化学习循环中为模型生成数据提供提示。
    -   **测试集**：位于`/home/zhangwj/assignment5/data/gsm8k/test.jsonl`，包含了独立的、从未在训练中出现过的问题，用于在评估环节衡量模型的泛化能力。

##### 3.3 训练超参数与数据流设计

项目的成功很大程度上依赖于精细的超参数设置，它们共同定义了整个训练循环的节奏和规模。

-   **总训练步数**：训练将进行`n_grpo_steps=200`个完整的迭代。
-   **学习率**：优化器`AdamW`的学习率被设置为`1e-5`，这是一个相对较小的数值，适用于对大型预训练模型进行微调，以防止训练过程中的灾难性遗忘。
-   **批量处理策略**：
    -   **策略生成批次大小**：每一步会生成`rollout_batch_size=256`个解决方案。
    -   **分组大小**：为每个问题生成`group_size=8`个候选答案。这意味着每一步会从训练集中抽取`32`个独特的问题。
    -   **训练批次大小**：用于一次模型参数更新的总样本数与生成批次大小相同（`train_batch_size=256`），这体现了**在线策略**的特点。
    -   **梯度累积**：为了在显存有限的情况下处理大型批次，我们采用梯度累积，将训练批次分解为`gradient_accumulation_steps=128`个**微批次**，每个微批次仅包含`2`个样本。模型会依次计算每个微批次的梯度，累积后再进行一次统一的参数更新。

-   **采样参数**：在生成答案时，采样温度设置为`sampling_temperature=0.7`，以鼓励模型生成多样化的回答。

##### 3.4 策略生成与奖励评估

-   **数据生成**：在每一步中，vLLM引擎根据当前策略，为32个问题中的每一个生成8个候选解决方案。生成过程中，通过一个包含`<think>`和`<answer>`标签的系统提示，来强制模型输出结构化的答案。
-   **奖励机制**：生成的答案由`r1_zero_reward_fn`函数进行评估。这是一个**稀疏的二进制奖励函数**：只有当答案**格式正确**且**最终答案正确**时，奖励才为`1`；否则奖励为`0`。
-   **优势估计**：为了使训练信号更加稳定，我们计算了**优势函数**。对于同一个问题生成的8个答案，其优势值被定义为：**该答案的奖励值 - 这一组8个答案的平均奖励值**。此外，为进一步降低方差，优势值还被除以该组奖励的标准差（并加入一个`1e-6`的**`advantage_eps`**防止除零错误）。这种分组归一化方法使模型能够学习“哪个答案比同一问题下的其他答案更好”，而非简单地判断“这个答案是否好”。

##### 3.5 模型更新与损失函数

-   **损失函数**：模型参数的更新基于`reinforce_with_baseline`损失类型。该损失将每个生成的标记（token）的对数概率与其对应的优势值相乘。通过优化该损失，模型将增加生成高优势答案所包含标记的概率，从而提升整体性能。
-   **掩码应用**：一个关键的实现细节是使用了**响应掩码（response_mask）**。该掩码确保损失函数和梯度只作用于模型生成的答案部分，而忽略了输入的提示部分，这是策略梯度方法正确实施的必要条件。

##### 3.6 策略同步与性能监控

-   **严格的在线策略**：项目采用了一种严格的在线策略执行方式。在每一步训练完成后，vLLM引擎都会被重建，并加载最新的模型权重（`refresh_vllm_every=1`）。这种高频率的同步确保了所有用于训练的样本都来自于最新的、最优的策略，是训练循环有效性的根本保障。
-   **实时评估**：同样，在每一步训练后，模型都会在独立的测试数据集上进行评估（`eval_every=1`）。评估指标包括：
    -   **`eval_accuracy`**：正确解题且格式正确的答案比例。
    -   **`eval_format_ok_rate`**：仅仅满足格式要求的答案比例。
-   **日志与可视化**：所有训练和评估的关键指标都会被记录到`./grpo_run/train_log.json`文件中。同时，系统还会动态生成并更新`./grpo_run/eval_curve.png`图像，清晰地展示模型在训练过程中的性能曲线。

---

#### 4. 结论与成果总结

该项目成功地构建并运行了一个完整的、高度优化的强化学习微调流水线。通过采用双GPU架构、细致的批处理策略（包括梯度累积）、分组优势估计和严格的在线策略同步机制，我们有效克服了强化学习训练中常见的稳定性和效率挑战。最终，我们获得了在数学推理和结构化输出控制方面都表现出色的模型，证明了这种方法在对齐和微调大语言模型方面的有效性和鲁棒性。所有关键的参数、过程和结果均已详实记录，为未来的进一步研究和应用提供了坚实的基础。